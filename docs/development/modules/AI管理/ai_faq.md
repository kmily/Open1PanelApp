# AI管理模块FAQ

## 常见问题

### Q1: 什么是Ollama？

**A**: Ollama是一个开源的大语言模型运行框架，支持在本地运行各种开源大语言模型，如：
- Llama 2/3
- Mistral
- Gemma
- Qwen
- 等更多模型

**相关代码**: [ai_v2.dart](../../../lib/api/v2/ai_v2.dart)

### Q2: 如何下载Ollama模型？

**A**: 模型下载步骤：
1. 进入AI管理 > Ollama模型页面
2. 点击"下载模型"按钮
3. 输入模型名称（如：llama2）
4. 选择模型版本
5. 点击"开始下载"
6. 等待下载完成

**API端点**: `POST /ai/ollama/model`

### Q3: 如何加载/卸载模型？

**A**: 模型操作步骤：
1. 在模型列表中找到目标模型
2. 点击"加载"按钮加载模型到内存
3. 点击"卸载"按钮释放模型内存

**API端点**: 
- 加载: `POST /ai/ollama/model/load`
- 卸载: `POST /ai/ollama/close`

### Q4: 什么是MCP服务器？

**A**: MCP（Model Context Protocol）服务器是一种标准化的AI服务接口，用于：
- 提供模型上下文管理
- 支持工具调用
- 实现多模型协作
- 标准化API接口

### Q5: 如何创建MCP服务器？

**A**: MCP服务器创建步骤：
1. 进入AI管理 > MCP服务器页面
2. 点击"创建服务器"按钮
3. 填写服务器名称和配置
4. 选择关联的模型
5. 配置端口和资源限制
6. 点击"创建"完成

**API端点**: `POST /ai/mcp/server`

### Q6: 如何查看GPU使用情况？

**A**: GPU监控步骤：
1. 进入AI管理页面
2. 查看GPU监控卡片
3. 显示GPU型号、显存、温度等信息
4. 实时更新使用状态

**API端点**: `GET /ai/gpu/load`

### Q7: 如何绑定域名？

**A**: 域名绑定步骤：
1. 进入AI管理 > 域名配置
2. 选择Ollama或MCP服务
3. 输入域名
4. 选择SSL证书（可选）
5. 配置IP白名单
6. 点击"绑定"完成

**API端点**: `POST /ai/domain/bind`

### Q8: 模型下载很慢怎么办？

**A**: 优化下载速度：
1. 检查网络连接
2. 使用镜像源
3. 选择较小的模型版本
4. 使用离线导入功能

### Q9: 没有GPU可以使用吗？

**A**: 可以使用CPU模式：
- 性能较慢但功能完整
- 适合小模型测试
- 建议使用量化模型

### Q10: 如何删除模型？

**A**: 删除模型步骤：
1. 在模型列表中找到目标模型
2. 确保模型已卸载
3. 点击"删除"按钮
4. 确认删除操作

**注意**: 删除后模型文件将被清理，需要重新下载。

**API端点**: `POST /ai/ollama/model/del`

## 故障排查指南

### 问题1: 模型加载失败

**可能原因**:
- 内存不足
- 模型文件损坏
- GPU资源不足

**排查步骤**:
1. 检查系统内存
2. 重新下载模型
3. 查看GPU显存
4. 尝试CPU模式

### 问题2: GPU识别失败

**可能原因**:
- 驱动未安装
- GPU不支持
- 权限问题

**排查步骤**:
1. 检查GPU驱动
2. 验证GPU型号
3. 检查用户权限
4. 查看系统日志

### 问题3: MCP服务器启动失败

**可能原因**:
- 端口被占用
- 配置错误
- 资源不足

**排查步骤**:
1. 检查端口占用
2. 验证配置文件
3. 查看服务器日志
4. 检查资源限制

### 问题4: 域名绑定失败

**可能原因**:
- DNS未解析
- 端口未开放
- SSL证书问题

**排查步骤**:
1. 检查DNS解析
2. 验证端口开放
3. 检查SSL证书
4. 查看防火墙规则

### 问题5: 模型响应慢

**可能原因**:
- GPU资源不足
- 模型过大
- 并发请求过多

**排查步骤**:
1. 检查GPU利用率
2. 尝试小模型
3. 减少并发请求
4. 增加资源限制

## 最佳实践建议

### 开发建议

1. **使用Provider管理状态**
   ```dart
   class AIProvider extends ChangeNotifier {
     List<OllamaModel> _models = [];
     List<GpuInfo> _gpuInfo = [];
     
     List<OllamaModel> get models => _models;
     List<GpuInfo> get gpuInfo => _gpuInfo;
     
     Future<void> loadModels() async {
       // 加载模型列表
     }
   }
   ```

2. **实现下载进度跟踪**
   ```dart
   Stream<double> downloadModel(String name) async* {
     // 返回下载进度
   }
   ```

3. **GPU监控刷新**
   - 定时刷新GPU状态
   - 实现自动刷新机制
   - 提供手动刷新按钮

### 运维建议

1. **资源管理**
   - 监控GPU温度
   - 控制模型数量
   - 定期清理无用模型

2. **性能优化**
   - 选择合适的模型大小
   - 使用量化模型
   - 合理分配资源

3. **安全配置**
   - 配置IP白名单
   - 启用HTTPS
   - 定期更新模型

### 使用建议

1. **模型选择**
   - 根据任务选择模型
   - 考虑资源限制
   - 测试模型效果

2. **资源规划**
   - 预留足够内存
   - 监控GPU使用
   - 合理分配并发

3. **故障预防**
   - 定期备份配置
   - 监控服务状态
   - 准备应急预案

---

**文档版本**: 1.0  
**最后更新**: 2026-02-14  
**维护者**: Open1Panel开发团队
